import streamlit as st
import requests
import json

st.set_page_config(page_title="Walidator Wniosk√≥w", page_icon="üìã", layout="wide")

OLLAMA_HOST = "http://ollama:11434"
DEFAULT_MODEL = "gpt-oss:120b"
ALTERNATIVE_MODEL = "gpt-oss:20b"

st.title("üìã Walidator Wniosk√≥w Grantowych")
st.markdown("Narzƒôdzie do wstƒôpnej oceny zgodno≈õci projektu z profilem instytutu oraz potencja≈Çem komercjalizacyjnym.")

# Sidebar for settings
with st.sidebar:
    st.header("Settings")
    try:
        response = requests.get(f"{OLLAMA_HOST}/api/tags")
        if response.status_code == 200:
            models = [m['name'] for m in response.json().get('models', [])]
            
            if DEFAULT_MODEL not in models:
                 models.append(DEFAULT_MODEL)
            if ALTERNATIVE_MODEL not in models:
                 models.append(ALTERNATIVE_MODEL)
            
            models.sort(key=lambda x: (x != DEFAULT_MODEL, x != ALTERNATIVE_MODEL))
            
            selected_model = st.selectbox("Select Model", models)
        else:
            st.error("Could not fetch models from Ollama.")
            selected_model = DEFAULT_MODEL
    except Exception as e:
        st.error(f"Connection error: {e}")
        selected_model = DEFAULT_MODEL
    
    st.markdown("---")
    st.markdown(f"- Active Model: `{selected_model}`")

# --- INPUT FORM ---
with st.container():
    c1, c2 = st.columns([1, 1])
    
    with c1:
        st.subheader("1. Profil Instytutu")
        institute_profile = st.text_area(
            "Opisz profil dzia≈Çalno≈õci badawczej i kompetencje instytutu:",
            height=300,
            placeholder="Np. Instytut specjalizuje siƒô w badaniach nad sztucznƒÖ inteligencjƒÖ, uczeniem maszynowym oraz ich zastosowaniem w medycynie..."
        )

    with c2:
        st.subheader("2. Dane Projektu")
        project_goal = st.text_area("Cel projektu (kr√≥tki opis):", height=100)
        innovations = st.text_area("G≈Ç√≥wne funkcjonalno≈õci / Cechy innowacyjne:", height=100, help="Elementy innowacyjne lub znaczƒÖco ulepszone w stosunku do rynku.")
        results = st.text_area("Rezultaty:", height=100, help="Mierzalne efekty, w tym komercjalizacja.")

# --- VALIDATION LOGIC ---
if st.button("Sprawd≈∫ Wniosek (GO / NO-GO)", type="primary"):
    if not all([institute_profile, project_goal, innovations, results]):
        st.error("Proszƒô wype≈Çniƒá wszystkie pola formularza.")
    else:
        with st.spinner(f"Analizujƒô wniosek ({selected_model})..."):
            # Construct the Prompt - MARKER-BASED FORMAT
            prompt = f"""Jeste≈õ surowym i precyzyjnym ekspertem oceniajƒÖcym wnioski grantowe.

DANE WEJ≈öCIOWE:
1. PROFIL INSTYTUTU:
{institute_profile}

2. CEL PROJEKTU:
{project_goal}

3. INNOWACJE:
{innovations}

4. REZULTATY:
{results}

KRYTERIA OCENY:
1. DOPASOWANIE DO PROFILU: Czy projekt mie≈õci siƒô w obszarze badawczym i kompetencyjnym instytutu?
2. KOMERCJALIZACJA: Czy wyniki prowadzƒÖ do rynkowej komercjalizacji (sprzeda≈º, licencja), a nie tylko "wdro≈ºenia w≈Çasnego"?

WYMAGANY FORMAT ODPOWIEDZI:

### 1. Analiza Zgodno≈õci z Profilem
(Twoja analiza...)

### 2. Analiza Potencja≈Çu Komercjalizacyjnego
(Twoja analiza...)

### UZASADNIENIE
(Kr√≥tkie uzasadnienie decyzji...)

NA SAMYM KO≈ÉCU ODPOWIEDZI MUSISZ UMIE≈öCIƒÜ DOK≈ÅADNIE JEDEN Z PONI≈ªSZYCH ZNACZNIK√ìW (skopiuj go dok≈Çadnie):
<<<WERDYKT: GO>>>
lub
<<<WERDYKT: NO-GO>>>

WA≈ªNE: Znacznik musi byƒá ostatniƒÖ liniƒÖ odpowiedzi, dok≈Çadnie w tym formacie z trzema nawiasami ostrymi.
"""
            
            payload = {
                "model": selected_model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.2,
                    "num_ctx": 4096
                }
            }
            
            try:
                response = requests.post(f"{OLLAMA_HOST}/api/generate", json=payload)
                if response.status_code == 200:
                    result_text = response.json().get('response', '')
                    
                    # Display Results
                    st.markdown("---")
                    st.subheader("Wynik Analizy AI")
                    st.markdown(result_text)
                    
                    # Extract verdict from marker - check NO-GO first (more specific)
                    if "<<<WERDYKT: NO-GO>>>" in result_text:
                        st.error("WERDYKT: NO-GO üõë")
                    elif "<<<WERDYKT: GO>>>" in result_text:
                        st.success("WERDYKT: GO ‚úÖ")
                    else:
                        st.warning("‚ö†Ô∏è Nie znaleziono znacznika werdyktu. Sprawd≈∫ tekst analizy.")
                    
                else:
                    st.error(f"B≈ÇƒÖd komunikacji z modelem: {response.text}")
            except Exception as e:
                st.error(f"WystƒÖpi≈Ç b≈ÇƒÖd: {e}")
